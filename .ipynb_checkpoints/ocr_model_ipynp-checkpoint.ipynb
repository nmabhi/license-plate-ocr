{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHgAK5QcOQJT"
   },
   "source": [
    "Mount google drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJKeyTUqLKOk",
    "outputId": "01091f1d-0d7a-4323-d644-78f6d93bd6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/license_plate_ocr\n",
      "5cnn_model.h5  8cnn_128lstm_model.h5  hdr      normal\t   Untitled.ipynb\n",
      "7cnn_model.h5  dataset.csv\t      hdr.zip  normal.zip\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/license_plate_ocr/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhaOgSgzRXFs"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tbp02BEaMgjs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZOBHL-HScKK"
   },
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WdLD6xMhSeLg"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\",names=[\"images\",\"labels\"])\n",
    "dataset[\"images\"] = dataset[\"images\"].apply(lambda s:\"hdr/\"+s if \"crop_h\" in s else \"normal/\"+s)\n",
    "dataset[\"type\"] = dataset[\"images\"].apply(lambda s:\"hd\" if \"crop_h\" in s else \"normal\")\n",
    "\n",
    "images = dataset[\"images\"]\n",
    "labels = dataset[\"labels\"]\n",
    "\n",
    "max_length = max([len(label) for label in labels])\n",
    "dataset[\"labels\"]= dataset[\"labels\"].apply(lambda s:s+(max_length-len(s))*\"#\")\n",
    "\n",
    "labels = dataset[\"labels\"]\n",
    "\n",
    "characters = set(char for label in labels for char in label)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "img_width = 200\n",
    "img_height = 50\n",
    "\n",
    "downsample_factor = 4\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0_buVKUakL_"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-XEvBPFeSik9"
   },
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=42)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split( X_train,y_train ,test_size=0.10, random_state=42)\n",
    "\n",
    "def split_data(images, labels, train_size=0.8, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "\n",
    "\n",
    "    return {\"image\": img, \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sC29oIHa4po"
   },
   "source": [
    "Create Test Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WH-BCFmLav37"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "  train_dataset.map(\n",
    "       encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NogM0gm24OPl"
   },
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOyNJ9S2a-DI",
    "outputId": "befa4527-74b2-4904-c450-0b9ae90e23ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ctc_loss/Shape:0\", shape=(2,), dtype=int32)\n",
      "Model: \"ocr_model_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 200, 50, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 200, 50, 64)  640         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv2D)                  (None, 200, 50, 64)  36928       Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 100, 25, 64)  0           Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv2D)                  (None, 100, 25, 64)  36928       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv2D)                  (None, 100, 25, 64)  36928       Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 50, 12, 64)   0           Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv2D)                  (None, 50, 12, 64)   36928       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv6 (Conv2D)                  (None, 50, 12, 64)   36928       Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 25, 6, 64)    0           Conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv7 (Conv2D)                  (None, 25, 6, 64)    36928       pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 12, 3, 64)    0           Conv7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 12, 192)      0           pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 12, 64)       12352       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 12, 64)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 12, 256)      197632      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 12, 256)      394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 12, 33)       8481        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (CTCLayer)             (None, 12, 33)       0           label[0][0]                      \n",
      "                                                                 dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 834,913\n",
      "Trainable params: 834,913\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "      \n",
    "        print(tf.shape(y_true))\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        \n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "       \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    \n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Third conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv3\",\n",
    "    )(x)\n",
    "   \n",
    "\n",
    "    # Furth conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv4\",\n",
    "    )(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "    # Batch normalization layer\n",
    "    batch_norm_4 = layers.BatchNormalization()\n",
    "    \n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv5\",\n",
    "    )(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv6\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool3\")(x)\n",
    "    \n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv7\",\n",
    "    )(x)\n",
    "    \n",
    "\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool4\")(x)\n",
    "\n",
    "    # Batch normalization layer\n",
    "    batch_norm_5 = layers.BatchNormalization()\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 16), (img_height // 16) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDLLUJGn4TJi"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukbua47yN045",
    "outputId": "b144ed23-c70d-45b2-af04-7904d5098a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"ocr_model_v1/ctc_loss/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"ocr_model_v1/ctc_loss/Shape:0\", shape=(2,), dtype=int32)\n",
      "33/33 [==============================] - ETA: 0s - loss: 23.4314Tensor(\"ocr_model_v1/ctc_loss/Shape:0\", shape=(2,), dtype=int32)\n",
      "33/33 [==============================] - 107s 3s/step - loss: 23.3570 - val_loss: 18.9394\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 19.0915 - val_loss: 18.5235\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 18.7281 - val_loss: 18.2129\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 18.4638 - val_loss: 17.9562\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 18.2077 - val_loss: 17.7656\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 3s 98ms/step - loss: 17.9875 - val_loss: 17.5216\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 17.7707 - val_loss: 17.4065\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 17.6655 - val_loss: 17.3519\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 17.6246 - val_loss: 17.3457\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 17.5820 - val_loss: 17.2719\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 17.5379 - val_loss: 17.2729\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 17.5269 - val_loss: 17.2289\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 17.5065 - val_loss: 17.2076\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 17.4234 - val_loss: 17.0907\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 17.3090 - val_loss: 16.8044\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 16.8754 - val_loss: 15.5536\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 15.7739 - val_loss: 14.2231\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 3s 97ms/step - loss: 14.4066 - val_loss: 12.4528\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 12.8999 - val_loss: 11.0506\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 11.2181 - val_loss: 9.8255\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 3s 97ms/step - loss: 9.9066 - val_loss: 7.8722\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 8.3821 - val_loss: 6.9411\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 3s 90ms/step - loss: 7.4046 - val_loss: 6.8209\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 6.6345 - val_loss: 6.2978\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 5.7379 - val_loss: 5.7068\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 5.2018 - val_loss: 5.8435\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 4.7978 - val_loss: 4.5688\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 4.1698 - val_loss: 4.6560\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 3.8574 - val_loss: 3.9108\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 3.5200 - val_loss: 4.0130\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 3.1919 - val_loss: 3.5984\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 2.6531 - val_loss: 3.7275\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 2.6101 - val_loss: 3.3755\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 2.1292 - val_loss: 3.3504\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 1.9866 - val_loss: 3.1472\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 1.9159 - val_loss: 2.8181\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 1.6540 - val_loss: 2.8915\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 3s 91ms/step - loss: 1.4985 - val_loss: 2.5864\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 1.3191 - val_loss: 2.5763\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 1.0574 - val_loss: 2.3705\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.9501 - val_loss: 2.2194\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.8878 - val_loss: 2.3255\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.7986 - val_loss: 2.2755\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.7967 - val_loss: 2.2403\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.7627 - val_loss: 2.3165\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.6198 - val_loss: 2.0632\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.5801 - val_loss: 1.9929\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.5784 - val_loss: 2.2104\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.5943 - val_loss: 2.0539\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.5513 - val_loss: 1.9554\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 3s 90ms/step - loss: 0.4567 - val_loss: 2.2056\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.5067 - val_loss: 2.1217\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.4909 - val_loss: 2.1715\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.4716 - val_loss: 1.9906\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.3833 - val_loss: 1.8203\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.3176 - val_loss: 1.9921\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.3294 - val_loss: 1.9030\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2868 - val_loss: 1.7333\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.3647 - val_loss: 1.8175\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.3541 - val_loss: 2.1655\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 3s 91ms/step - loss: 0.4335 - val_loss: 1.8900\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 3s 97ms/step - loss: 0.3353 - val_loss: 2.0136\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2424 - val_loss: 1.8736\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2176 - val_loss: 1.7955\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2191 - val_loss: 1.8279\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.2067 - val_loss: 1.7341\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.2031 - val_loss: 1.8808\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.2097 - val_loss: 1.7184\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.1716 - val_loss: 1.9778\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.1935 - val_loss: 1.9461\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2067 - val_loss: 1.6963\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2049 - val_loss: 1.7809\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.1538 - val_loss: 1.6078\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.2055 - val_loss: 1.7271\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 3s 93ms/step - loss: 0.2186 - val_loss: 2.2382\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.3436 - val_loss: 2.0976\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.3116 - val_loss: 1.9835\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 3s 96ms/step - loss: 0.3237 - val_loss: 1.5180\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2554 - val_loss: 2.1024\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.5169 - val_loss: 1.9519\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.3757 - val_loss: 2.0589\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2312 - val_loss: 1.9155\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.2071 - val_loss: 1.9257\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.1984 - val_loss: 1.8172\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 3s 92ms/step - loss: 0.1220 - val_loss: 1.7167\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.1258 - val_loss: 1.6690\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 3s 94ms/step - loss: 0.0911 - val_loss: 1.5378\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.0997 - val_loss: 1.7225\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIlPqrtI4Wcj"
   },
   "source": [
    "Predictions on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cStXDTTPh7j",
    "outputId": "718cfa85-3b4c-4ea7-9a60-3dc610c59848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 200, 50, 1)]      0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 200, 50, 64)       640       \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 200, 50, 64)       36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 100, 25, 64)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 100, 25, 64)       36928     \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 100, 25, 64)       36928     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 50, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 50, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "Conv6 (Conv2D)               (None, 50, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 25, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "Conv7 (Conv2D)               (None, 25, 6, 64)         36928     \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 12, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 12, 192)           0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 12, 64)            12352     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 12, 256)           197632    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 12, 33)            8481      \n",
      "=================================================================\n",
      "Total params: 834,913\n",
      "Trainable params: 834,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction model by extracting layers till the output layer\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "\n",
    "prediction_model.summary()\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "predictions = []\n",
    "orig_texts = []\n",
    "for batch in validation_dataset:\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    predictions = predictions + pred_texts\n",
    "\n",
    "    \n",
    "    for label in batch_labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X2jb-9m4chT"
   },
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tIR-R2jKIaRk"
   },
   "outputs": [],
   "source": [
    "prediction_model.save(\"7cnn_128lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB5W57mK4e42"
   },
   "source": [
    "Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SaukXVtRP0Za"
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()\n",
    "output_df[\"predicted_text\"] = predictions\n",
    "output_df[\"original_text\"] = orig_texts\n",
    "output_df[\"predicted_text\"]=output_df[\"predicted_text\"].apply(lambda s:s.replace(\"[UNK]\",\"\"))\n",
    "output_df[\"original_text\"]=output_df[\"original_text\"].apply(lambda s:s.replace(\"[UNK]\",\"\"))\n",
    "output_df.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWqowsXvjrdQ",
    "outputId": "d017ebab-2c62-4c92-c637-54907125fd57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     110\n",
       "False     21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(output_df[\"predicted_text\"]==output_df[\"original_text\"]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ocr_model.ipynp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
